多少数据来需要分布式

# 核心的数据量超过10T
行核心场景要达到什么客观条件才适合上分布式数据库，如何评估整体成本？

A1：这个其实没有太统一的标准，通过我们的验证，可以说一个大概的数字供大家进行简单参考。如果核心的数据量超过10T，Oracle的承载能力可能会受到一定的限制。也就是说，核心数据量在10T以上，一般就可以去考虑上分布式，或者说，这已经达到了一个迫切的程度。因为随着数据量的增长，可能在之后， Oracle这种单体的商业型数据库支撑起数据会越来越吃力。



这个数据量比起动辄PB的自然是小巫见大巫，所以才会上来问这个问题。如果数据规模(GB? TB?)并不是唯一的衡量尺度那还有什么其他标准呢？



。定量就是一般数据或者未来的数据量会达到PB级别（可能GB）或以上就要用分布式，当然前提也是你的处理逻辑可以进行分布式。


麦肯锡对于“大数据”的定义是：一种规模大到在获取、存储、管理、分析方面大大的超出了传统数据库软件工具能力范围的数据集合，具有4V特征，即Volumn（海量的规模）、Velocity（快速的流转）、Variety（多样的类型）和Value（低密度的价值）。“大”指的是数据规模，大数据一般指在10TB(1TB=1024GB)规模以上的数据量。

，首先，我们知道，在大数据出现之前，我们对数据的日常处理分析常常使用的是诸如sqlsever/oracle/mysql等传统关系数据库，处理T级别的数据量已经是这些数据库的极限，面对PB/EB/ZB级的数据量那就更无能为力了



我觉得这个问题没有一个具体答案。光从数据规模方面来说，当达到PB，TB级别时，才被称为大数据。       hadoop主要是用来对海量数据进行存储和全量分析的。它本身是一个分布式系统，核心由分布式文件系统hdfs，和分布式计算框架mapreduce组成，在存储和计算时能够发挥出集群中每台机器的能力。

作者：happy
链接：https://www.zhihu.com/question/21975289/answer/20005562
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。